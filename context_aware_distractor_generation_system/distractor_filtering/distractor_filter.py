import logging
import pickle
import time
from pathlib import Path

import spacy
import torch
from transformers import BertTokenizer, BertForMaskedLM


class DistractorFilter:
    """
    A unified filter that can screen distractors using trigrams, dependency relations,
    and BERT-based sentence plausibility (Pseudo-Log-Likelihood).
    """

    def __init__(self, trigram_path, dependency_index_path,
                 bert_model_name='cl-tohoku/bert-base-japanese-whole-word-masking'):
        """
        Initializes the filter by loading all required data sets and models.
        """
        self.logger = logging.getLogger('DistractorFilter')

        # --- Load Trigram and Dependency Data ---
        self.trigrams = self._load_pickle_data(
            Path(trigram_path),
            "trigrams",
            empty_type=set()
        )

        self.dependency_index = self._load_pickle_data(
            Path(dependency_index_path),
            "dependency index",
            empty_type={}
        )
        # --- Load spaCy/GiNZA for Dependency Parsing ---
        self.logger.info("Loading spaCy/GiNZA model for live parsing...")
        try:
            self.nlp = spacy.load("ja_ginza")
            self.logger.info("âœ… GiNZA model loaded successfully.")
        except Exception as e:
            self.logger.error(f"Failed to load GiNZA model: {e}", exc_info=True)
            self.nlp = None

        # --- Load BERT Model for PPL Filtering ---
        self.logger.info(f"Loading BERT model '{bert_model_name}' for filtering...")
        try:
            self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)
            self.bert_model = BertForMaskedLM.from_pretrained(bert_model_name)
            self.bert_model.eval()
            self.logger.info("âœ… BERT model loaded successfully for filtering.")
        except Exception as e:
            self.logger.error(f"Failed to load BERT model: {e}", exc_info=True)
            self.bert_tokenizer = None
            self.bert_model = None

    def _load_pickle_data(self, file_path, data_name, empty_type):
        """
        Helper method to load a pickled data object from a file.
        Returns a given empty object on failure.

        Args:
            file_path (Path): Path to the pickle file.
            data_name (str): Name for logging.
            empty_type: The empty object to return on failure (e.g., set(), {}).
        """
        if not file_path.exists():
            self.logger.error(f"{data_name.capitalize()} data file not found: {file_path}")
            return empty_type

        self.logger.info(f"Loading {data_name} from {file_path}...")
        start_time = time.time()
        try:
            with open(file_path, 'rb') as f:
                data_obj = pickle.load(f)
            duration = time.time() - start_time
            self.logger.info(
                f"Successfully loaded {len(data_obj):,} items into a {type(data_obj).__name__} in {duration:.2f}s.")
            return data_obj
        except Exception as e:
            self.logger.error(f"Failed to load {data_name} data: {e}", exc_info=True)
            return empty_type

    def filter_by_trigram(self, candidates: list[str], prev_word: str, next_word: str) -> tuple[list[str], list[str]]:
        if not self.trigrams:
            self.logger.warning("Trigram set is empty. Skipping trigram filtering.")
            return candidates, []

        accepted, rejected = [], []
        for candidate in candidates:
            trigram_to_check = (prev_word, candidate, next_word)
            if trigram_to_check not in self.trigrams:
                accepted.append(candidate)
            else:
                rejected.append(candidate)

        return accepted, rejected

    def filter_by_dependency(self, candidates: list[str], sentence_template: str, blank_placeholder: str = "___") -> \
            tuple[list[str], list[str]]:
        if not self.dependency_index or not self.nlp:
            self.logger.warning("Dependency index or GiNZA model not available. Skipping dependency filtering.")
            return candidates, []

        accepted, rejected = [], []
        for candidate in candidates:
            full_sentence = sentence_template.replace(blank_placeholder, candidate, 1)
            is_rejected = False
            try:
                doc = self.nlp(full_sentence)
                for token in doc:
                    # We only care about relations where the candidate is the head or the child
                    if token.lemma_ != candidate and token.head.lemma_ != candidate:
                        continue

                    live_relation = (token.dep_, token.head.lemma_, token.lemma_)
                    # Efficiently check if the head of the relation is in our index
                    known_relations_for_head = self.dependency_index.get(live_relation[1], [])

                    if live_relation in known_relations_for_head:
                        is_rejected = True
                        rejected.append(candidate)
                        break  # Found a reason to reject, move to next candidate

                if not is_rejected:
                    accepted.append(candidate)

            except Exception as e:
                self.logger.error(f"Error during dependency parsing for candidate '{candidate}': {e}")
                accepted.append(candidate)  # Failsafe: accept if parsing fails

        return accepted, rejected

    def _calculate_pseudo_log_likelihood(self, sentence: str) -> float:
        """
        Calculates sentence PLL using the PLL-word-l2r method.
        This method corrects for the score inflation of multi-token words by
        also masking subsequent sub-tokens of the same word.
        """
        if not self.bert_model or not self.bert_tokenizer:
            print("Error: BERT model or tokenizer not initialized.")
            return -float('inf')

        tensor_input = self.bert_tokenizer.encode(sentence, return_tensors='pt')

        # Check if the tensor is 2D ([batch_size, sequence_length]), if not we need to add a batch dimension
        if len(tensor_input.shape) == 1:
            tensor_input = tensor_input.unsqueeze(0)

        # To identify which tokens belong to the same word, we need their string representation. -> OOV tokens or complex tokens
        # WordPiece tokenizers (used by BERT) mark sub-tokens of a word with a '##' prefix.
        # Example: "tokenization" -> ["token", "##ization"]
        token_ids = tensor_input[0].tolist()
        tokens = self.bert_tokenizer.convert_ids_to_tokens(token_ids)

        total_log_likelihood = 0.0

        # We iterate through each token in the sentence to calculate its individual PLL.
        # We skip the first token ([CLS]) and the last token ([SEP])
        for i in range(1, tensor_input.shape[1] - 1):
            # Store the original token ID at the current position `i`. We'll need this later
            # to find its log-likelihood in the model's output.
            original_token_id = tensor_input[0, i].item()

            masked_input = tensor_input.clone()

            # 1. Mask the current target token at position `i`.
            # We replace its ID with the special [MASK] token ID.
            masked_input[0, i] = self.bert_tokenizer.mask_token_id

            # 2. Mask all subsequent tokens that are part of the *same word*.
            # We check tokens to the right of the current one (`i + 1`).
            for j in range(i + 1, tensor_input.shape[1] - 1):
                # If a token string starts with '##', it's a continuation of the previous token's word.
                if tokens[j].startswith('##'):
                    # If it's part of the same word, mask it as well.
                    masked_input[0, j] = self.bert_tokenizer.mask_token_id
                else:
                    # If we encounter a token that does NOT start with '##', it means we've
                    # reached the beginning of a new word. We stop masking.
                    break

            # Performance optimization that disables gradient calculation because we are only doing inference, not training.
            with torch.no_grad():
                outputs = self.bert_model(masked_input)

            # [0] -> Selects the results for the first sentence in our batch
            # [i] -> Get predictions for our target position `i`
            logits_for_target_token = outputs.logits[0, i]
            log_probs = torch.nn.functional.log_softmax(logits_for_target_token, dim=0)

            # From the distribution of log probabilities, we select the one corresponding
            # to our *original*, unmasked token. This value is the PLL for this single token.
            token_log_likelihood = log_probs[original_token_id].item()

            # Add this token's score to the running total for the sentence.
            total_log_likelihood += token_log_likelihood

        return total_log_likelihood

    def filter_by_bert(self, candidates: list[str], carrier_sentence: str, context_type: str) -> tuple[
        list[str], list[str]]:
        """
        Filters candidates by rejecting those that make the sentence "too plausible"
        according to BERT's PPL score.
        """
        if not self.bert_model:
            self.logger.warning("BERT model not available. Skipping BERT filtering.")
            return candidates, []
        if "___" not in carrier_sentence:
            self.logger.error("Carrier sentence for BERT filter must contain '___' placeholder.")
            return [], candidates

        # rejection_threshold = -25.0 if context_type == 'closed' else -15.0
        rejection_threshold = -100
        self.logger.info(
            f"--- Filtering with BERT (Context: {context_type}, Rejection Threshold > {rejection_threshold}) ---")

        accepted, rejected = [], []
        for candidate in candidates:
            full_sentence = carrier_sentence.replace("___", candidate)
            score = self._calculate_pseudo_log_likelihood(full_sentence)
            if score <= rejection_threshold:
                self.logger.info(f"  âœ… ACCEPTED: '{candidate}' (Score: {score:.2f})")
                accepted.append(candidate)
            else:
                self.logger.info(f"  âŒ REJECTED: '{candidate}' (Score: {score:.2f})")
                rejected.append(candidate)
        return accepted, rejected


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    print("Initializing the Unified DistractorFilter...")
    DATA_DIR = Path("../../re_implementation/processed_corpus_data")
    TRIGRAM_FILE = DATA_DIR / "jp_trigram_counts.pkl"
    DEPENDENCY_FILE = DATA_DIR / "jp_dependency_relations.pkl"

    unified_filter = DistractorFilter(TRIGRAM_FILE, DEPENDENCY_FILE)
    print("=" * 60)

    if unified_filter.bert_model:
        # --- SETUP: TEST CASES AND TRANSLATIONS ---
        test_cases: list[dict[str, any]] = [
            {
                "sentence": "å‹•ç‰©åœ’ã§ã€å¤§ããª___ãŒé¼»ã‚’é«˜ãä¸Šã’ã¦ã„ãŸã€‚",
                "target": "è±¡", "context": "closed",
                "english_sentence": "At the zoo, the big ___ was raising its trunk high.",
                "prev_word": "å¤§ããª", "next_word": "é¼»",
                "candidates": ["è±¡", "ã‚­ãƒªãƒ³", "ãƒžãƒ³ãƒ¢ã‚¹", "è»Š", "æœ¨", "ç…å­", "ã‚¢ãƒªã‚¯ã‚¤"],
                "translations": {"è±¡": "elephant", "ã‚­ãƒªãƒ³": "giraffe", "ãƒžãƒ³ãƒ¢ã‚¹": "mammoth", "è»Š": "car",
                                 "æœ¨": "tree", "ç…å­": "lion", "ã‚¢ãƒªã‚¯ã‚¤": "anteater"}
            },
            {
                "sentence": "å…¬åœ’ã§ã€ãŸãã•ã‚“ã®___ãŒéŠã‚“ã§ã„ãŸã€‚",
                "target": "å­ä¾›", "context": "open",
                "english_sentence": "At the park, many ___ were playing.",
                "prev_word": "ãŸãã•ã‚“ã®", "next_word": "éŠã‚“ã§ã„ãŸ",
                "candidates": ["å­ä¾›", "äººã€…", "å­¦ç”Ÿ", "å‹•ç‰©", "é¯‰", "æˆäºº", "è¦ª", "é£Ÿå“"],
                "translations": {"å­ä¾›": "child", "äººã€…": "people", "å­¦ç”Ÿ": "students", "å‹•ç‰©": "animals", "é¯‰": "carp",
                                 "æˆäºº": "adults", "è¦ª": "parents", "é£Ÿå“": "food"}
            },
            {
                "sentence": "å½¼ã¯100ãƒ¡ãƒ¼ãƒˆãƒ«ã‚’10ç§’ã§___ã“ã¨ãŒã§ãã‚‹ã€‚",
                "target": "èµ°ã‚‹", "context": "closed",
                "english_sentence": "He can ___ 100 meters in 10 seconds.",
                "prev_word": "10ç§’ã§", "next_word": "ã“ã¨",
                "candidates": ["èµ°ã‚‹", "æ­©ã", "æ³³ã", "é£›ã¶", "ã‚ã‚‹", "é£²ã‚€", "é£Ÿã¹ã‚‹", "æ­Œã†"],
                "translations": {"èµ°ã‚‹": "run", "æ­©ã": "walk", "æ³³ã": "swim", "é£›ã¶": "fly", "ã‚ã‚‹": "to be",
                                 "é£²ã‚€": "drink", "é£Ÿã¹ã‚‹": "eat", "æ­Œã†": "sing"}
            },
            {
                "sentence": "ã“ã®___ã¯ã¨ã¦ã‚‚é‡è¦ã§ã™ã€‚",
                "target": "å•é¡Œ", "context": "open",
                "english_sentence": "This ___ is very important.",
                "prev_word": "ã“ã®", "next_word": "ã¨ã¦ã‚‚",
                "candidates": ["å•é¡Œ", "ç‚¹", "éƒ¨åˆ†", "é¸æ‰‹", "è„š", "äºº", "ã‚¤ãƒ™ãƒ³ãƒˆ", "é£Ÿå“", "è³ªå•", "æ„›", "æ­Œã†",
                               "æ­Œ"],
                "translations": {"å•é¡Œ": "problem", "ç‚¹": "point", "éƒ¨åˆ†": "part", "é¸æ‰‹": "athlete", "è„š": "leg",
                                 "äºº": "person", "ã‚¤ãƒ™ãƒ³ãƒˆ": "event", "é£Ÿå“": "food", "è³ªå•": "question",
                                 "æ„›": "love", "æ­Œã†": "sing", "æ­Œ": "song"}
            },
            {
                "sentence": "ç§ã®___ã¯ã¨ã¦ã‚‚å¯æ„›ã„ã€‚", "target": "çŒ«", "context": "open",
                "english_sentence": "My ___ is very cute.",
                "prev_word": "ç§ã®", "next_word": "ã¨ã¦ã‚‚",
                "candidates": ["çŒ«", "çŠ¬", "å­ä¾›", "ãƒãƒ ã‚¹ã‚¿ãƒ¼", "ã‚«ãƒãƒ³"],
                "translations": {"çŒ«": "cat", "çŠ¬": "dog", "å­ä¾›": "child", "ãƒãƒ ã‚¹ã‚¿ãƒ¼": "hamster", "ã‚«ãƒãƒ³": "bag"}
            },
            {
                "sentence": "ç§ã®___ã¯å¯æ„›ãã¦ã€ã‚ˆããƒ‹ãƒ£ãƒ¼ã¨é³´ãã€‚", "target": "çŒ«", "context": "closed",
                "english_sentence": "My ___ is cute and meows a lot.",
                "prev_word": "ç§ã®", "next_word": "å¯æ„›ãã¦",
                "candidates": ["çŒ«", "çŠ¬", "é³¥", "å­çŠ¬", "å­çŒ«"],
                "translations": {"çŒ«": "cat", "çŠ¬": "dog", "é³¥": "bird", "å­çŠ¬": "puppy", "å­çŒ«": "kitten"}
            }
        ]

        print("=" * 70)

        # --- EXECUTION LOOP ---
        for i, case in enumerate(test_cases):
            english_sentence_with_blank = case["english_sentence"]

            print(f"ðŸ§ª TEST CASE {i + 1}: {case['context'].upper()} CONTEXT")
            print(f"   Sentence: {case['sentence']}")
            print(f"   English:  {english_sentence_with_blank}'")

            initial_candidates = case["candidates"]
            translations = case["translations"]


            def format_list(candidate_list):
                return [f"{c} ({translations.get(c, 'N/A')})" for c in candidate_list]


            print(f"   Initial Candidates: {format_list(initial_candidates)}")
            print("=" * 70)

            # Step 1: Trigram Filter
            print("\n--- 1. Applying Trigram Filter ---")
            remaining_after_trigram, rejected_by_trigram = unified_filter.filter_by_trigram(
                initial_candidates, case["prev_word"], case["next_word"]
            )
            print(f"   Rejected {len(rejected_by_trigram)} candidates: {format_list(rejected_by_trigram)}")
            print(f"   Candidates remaining: {format_list(remaining_after_trigram)}")
            print("-" * 50)

            # Step 2: Dependency Filter
            # print("\n--- 2. Applying Dependency Filter ---")
            # remaining_after_dep, rejected_by_dep = unified_filter.filter_by_dependency(
            #     remaining_after_trigram, case["sentence"]
            # )
            # print(f"   Rejected {len(rejected_by_dep)} candidates: {format_list(rejected_by_dep)}")
            # print(f"   Candidates remaining: {format_list(remaining_after_dep)}")
            # print("-" * 50)

            # Step 3: BERT Filter
            print("\n--- 3. Applying BERT Filter ---")
            final_distractors, rejected_by_bert = unified_filter.filter_by_bert(
                # remaining_after_dep, case["sentence"], case["context"]
                remaining_after_trigram, case["sentence"], case["context"]
            )
            print("-" * 50)

            # Final Results for this case
            print("\n" + "-" * 25 + f" RESULTS FOR CASE {i + 1} " + "-" * 25)
            print(f"{'Initial Candidates:':<28} {format_list(initial_candidates)}")
            print(f"{'Rejected by Trigram Filter:':<28} {format_list(rejected_by_trigram)}")
            # print(f"{'Rejected by Dependency Filter:':<28} {format_list(rejected_by_dep)}")
            print(f"{'Rejected by BERT Filter:':<28} {format_list(rejected_by_bert)}")
            print("-" * 35)
            print(f"{'Final Accepted Distractors:':<28} {format_list(final_distractors)}")
            print("=" * 70)
