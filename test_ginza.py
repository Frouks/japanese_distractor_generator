def test_ginza_tokenization():
    import spacy
    nlp = spacy.load("ja_ginza")

    # Test the exact cases from your log
    test_cases = [
        ("å½¼å¥³ã¯æ˜¨æ—¥ã€é•·ã„æ‰‹ç´™ã‚’æ›¸ã„ãŸã€‚", "æ›¸ã„ãŸ"),
        ("å½¼ã¯ãƒãƒ©ã‚½ãƒ³å¤§ä¼šã§é€Ÿãèµ°ã£ãŸã€‚", "èµ°ã£ãŸ"),
        ("ç§ã®çŒ«ã¯ã¨ã¦ã‚‚å¯æ„›ã„ã€‚", "çŒ«"),
        ("å¦»ãŒè²·ã£ãŸã‹ã°ã‚“ã‚’å‹é”ã«â¾’ã›ãŸã„ã€‚", "è²·ã£ãŸ"),
        ("ä»Šæ—¥ã¯ã¨ã¦ã‚‚å¯’ã‹ã£ãŸã€‚", "å¯’ã‹ã£ãŸ"),  # was cold
        ("å½¼å¥³ã¯ã¨ã¦ã‚‚ç¾ã—ã‹ã£ãŸã€‚", "ç¾ã—ã‹ã£ãŸ"),  # was beautiful
        ("ã“ã®æœ¬ã¯é¢ç™½ããªã„ã€‚", "é¢ç™½ããªã„"),  # not interesting
        ("æ˜¨æ—¥ã¯æš‘ããªã‹ã£ãŸã€‚", "æš‘ããªã‹ã£ãŸ"),  # was not hot
        ("ä»Šæ—¥ã¯å¯’ã„ã€‚", "å¯’ã„"),  # cold (basic form)
        ("æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚", "æ±äº¬")  # the capital of japan is tokyo
    ]

    for sentence, candidate in test_cases:
        print(f"\n=== Testing: '{candidate}' in '{sentence}' ===")
        doc = nlp(sentence)

        print("All tokens:")
        for token in doc:
            print(f"  '{token.text}' â†’ lemma: '{token.lemma_}'")
            print(
                f" '{token.text}' -> dependencies: '{token.dep_}' with head: '{token.head.lemma_}' and child: '{token.lemma_}'")

        print(f"\nLooking for candidate '{candidate}':")
        found_exact = False
        for token in doc:
            if token.text == candidate:
                print(f"  âœ… EXACT MATCH: '{token.text}' â†’ lemma: '{token.lemma_}'")
                found_exact = True

        if not found_exact:
            print(f"  âŒ NO EXACT MATCH for '{candidate}'")
            print("  â†’ Need to use fallback strategy")


def test_morphological_complexity():
    import spacy
    nlp = spacy.load("ja_ginza")

    print("=== THREE MORPHOLOGICAL COMPLEXITY EXAMPLES ===\n")

    # Example 1: Already in lemma form (no splitting)
    print("ğŸ”¹ EXAMPLE 1: Already in lemma form")
    sentence1 = "ç§ã¯æœ¬ã‚’èª­ã‚€ã€‚"
    candidate1 = "æœ¬"
    print(f"Sentence: {sentence1}")
    print(f"Candidate: {candidate1}")

    doc1 = nlp(sentence1)
    print("Tokenization:")
    for token in doc1:
        print(f"  '{token.text}' â†’ lemma: '{token.lemma_}' (POS: {token.pos_})")

    # Check if exact match works
    for token in doc1:
        if token.text == candidate1:
            print(f"âœ… EXACT MATCH: '{token.text}' â†’ lemma: '{token.lemma_}'")
    print()

    # Example 2: Split into 2 parts (stem + auxiliary)
    print("ğŸ”¹ EXAMPLE 2: Split into 2 parts")
    sentence2 = "å½¼å¥³ã¯æ‰‹ç´™ã‚’æ›¸ã„ãŸã€‚"
    candidate2 = "æ›¸ã„ãŸ"
    print(f"Sentence: {sentence2}")
    print(f"Candidate: {candidate2}")

    doc2 = nlp(sentence2)
    print("Tokenization:")
    for token in doc2:
        if token.pos_ in ['VERB', 'ADJ', 'AUX'] or token.text in ['æ›¸ã„', 'ãŸ']:
            print(f"  '{token.text}' â†’ lemma: '{token.lemma_}' (POS: {token.pos_})")

    # Show morphological matching
    print("Morphological matching:")
    for token in doc2:
        if (token.pos_ in ['VERB', 'ADJ'] and
                candidate2.startswith(token.text)):
            print(f"  ğŸ¯ STEM MATCH: '{token.text}' â†’ lemma: '{token.lemma_}'")
    print()

    # Example 3: Split into 3+ parts (complex morphology)
    print("ğŸ”¹ EXAMPLE 3: Split into 3+ parts")
    sentence3 = "æ˜¨æ—¥ã¯æš‘ããªã‹ã£ãŸã€‚"
    candidate3 = "æš‘ããªã‹ã£ãŸ"
    print(f"Sentence: {sentence3}")
    print(f"Candidate: {candidate3}")

    doc3 = nlp(sentence3)
    print("Tokenization:")
    for token in doc3:
        if token.pos_ in ['VERB', 'ADJ', 'AUX'] or token.text in ['æš‘ã', 'ãªã‹ã£', 'ãŸ']:
            print(f"  '{token.text}' â†’ lemma: '{token.lemma_}' (POS: {token.pos_})")

    # Show morphological matching
    print("Morphological matching:")
    for token in doc3:
        if (token.pos_ in ['VERB', 'ADJ'] and
                candidate3.startswith(token.text)):
            print(f"  ğŸ¯ STEM MATCH: '{token.text}' â†’ lemma: '{token.lemma_}'")
    print()

    # Summary
    print("=== SUMMARY ===")
    print("1. æœ¬ (lemma form) â†’ æœ¬ (no splitting, exact match works)")
    print("2. æ›¸ã„ãŸ (past tense) â†’ æ›¸ã„ + ãŸ (2 parts, need stem matching)")
    print("3. æš‘ããªã‹ã£ãŸ (negative past) â†’ æš‘ã + ãªã‹ã£ + ãŸ (3 parts, need stem matching)")
    print("\nOur algorithm should handle all three cases! ğŸ¯")


if __name__ == "__main__":
    test_ginza_tokenization()
    test_morphological_complexity()
